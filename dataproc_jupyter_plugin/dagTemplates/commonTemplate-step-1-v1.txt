# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from datetime import datetime, timedelta, timezone
import uuid
from airflow import DAG
from airflow.providers.google.cloud.operators.dataproc import DataprocCreateBatchOperator
from airflow.providers.google.cloud.operators.dataproc import DataprocSubmitJobOperator
from airflow.operators.python_operator import PythonOperator
from google.cloud import dataproc_v1
from google.api_core.client_options import ClientOptions
import os


default_args = {
    'owner': '{{owner}}',
    'start_date': '{{startDate}}', 
    'email': {{email}}, 
    'email_on_failure': '{{email_failure}}',     
    'email_on_retry': '{{email_delay}}',      
    'email_on_success':'{{email_success}}'
}

time_zone = '{{timeZone}}'

dag = DAG(
    '{{job_name}}', 
    default_args=default_args,
    description='{{job_name}}',
    tags =['dataproc_jupyter_plugin'],
    schedule_interval='{{scheduleInterval}}',
    catchup= False
)

#function to pass input notebook, output notebook path and parameters as arguments to task
def get_notebook_args(input_notebook,parameter,task_id):
    input_notebook = input_notebook
    output_notebook = {% raw %}"{{ ti.xcom_pull(task_ids='" + task_id + "',key='output_file_path') }}"{% endraw %}
    notebook_args = [input_notebook, output_notebook]
    parameters_str = '''
    {}
    '''.format(',\n'.join(parameter)).replace('\n', '')
    # Check if parameters is not empty or contains only whitespace
    if parameters_str.strip():  
        notebook_args.extend(["--parameters", parameters_str])
    return notebook_args

def get_client_cert():
    # code to load client certificate and private key.
    return client_cert_bytes, client_private_key_bytes


def get_cluster_state_start_if_not_running(cluster):

    options = ClientOptions(api_endpoint="{{gcpRegion}}-dataproc.googleapis.com:443",
    client_cert_source=get_client_cert)

    # Create a client
    client = dataproc_v1.ClusterControllerClient(client_options=options)

    # Initialize request argument(s)
    request = dataproc_v1.GetClusterRequest(
        project_id='{{gcpProjectId}}',
        region='{{gcpRegion}}',
        cluster_name=cluster,
    )

    # Make the request
    response = client.get_cluster(request=request)    
   
    # Handle the response
    print(f"State is {response.status.state}")
    if response.status.state in (6, 7):
        print("Cluster is in stopped/stopping state. Starting the cluster")
        request1 = dataproc_v1.StartClusterRequest(
            project_id='{{gcpProjectId}}',
            region='{{gcpRegion}}',
            cluster_name=cluster,
        )
        operation = client.start_cluster(request=request1)
        print("Waiting for operation to complete...")
        response = operation.result()
        if response.status.state in (2, 5):
            print("Cluster is started succesfully")    
    elif response.status.state in (2, 5):
       print("Cluster is already running")
    else:
        print("Cluster is unavailable")
        raise Exception("Cluster is unavailable")

 
def stop_the_cluster(cluster,stopStatus):
    if stopStatus == 'True':
        options = ClientOptions(api_endpoint="{{gcpRegion}}-dataproc.googleapis.com:443",
            client_cert_source=get_client_cert)
    
        # Create a client
        client = dataproc_v1.ClusterControllerClient(client_options=options)
    
        # Initialize request argument(s)
        request = dataproc_v1.StopClusterRequest(
            project_id='{{gcpProjectId}}',
            region='{{gcpRegion}}',
            cluster_name=cluster,
        )
    
        # Make the request
        operation = client.stop_cluster(request=request)
        print("Waiting for operation to complete...")
        response = operation.result()
        if response.status.state in (6, 7):
            print("Cluster is stopped succesfully")
    
        # Handle the response
        print(response)

def create_unique_output_file_path(run_id,task_id,output_notebook, **kwargs):
    output_file_path = f"{output_notebook}{run_id}-{task_id}.ipynb"
    print(output_file_path)
    kwargs['ti'].xcom_push(key='output_file_path', value=output_file_path)
    return output_file_path

def stop_multiple_clusters(cluster_stop):
    for cluster, stop in cluster_stop.items():
        stop_the_cluster(cluster, str(stop))


stop_cluster = PythonOperator(
    task_id='stop_clusters',
    python_callable=stop_multiple_clusters,
    op_args=[{{clusterStop}}],
    provide_context=True,
    dag=dag
)
    