# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from datetime import datetime, timedelta, timezone
from airflow import DAG
from airflow.providers.google.cloud.operators.dataproc import DataprocSubmitJobOperator
from airflow.operators.python_operator import PythonOperator
from google.cloud import dataproc_v1
from google.api_core.client_options import ClientOptions

default_args = {
    'owner': '{{owner}}',
    'start_date': '{{start_date}}',
    'email': {{email}}, 
    'email_on_failure': '{{email_failure}}',     
    'email_on_retry': '{{email_delay}}',     
    'email_on_success':'{{email_success}}'
}
dag = DAG(
    '{{name}}', 
    default_args=default_args,
    description='{{name}}',
    tags =['dataproc_jupyter_plugin'],
    schedule_interval='{{schedule_interval}}',
    catchup= False
)

def write_output_to_file(run_id,task_id, **kwargs):
    output_file_path = f"{{output_notebook}}{run_id}{task_id}.ipynb"
    print(output_file_path)
    kwargs['ti'].xcom_push(key='output_file_path', value=output_file_path)
    return output_file_path

def get_notebook_args(input_notebook,parameter,task_id):
    input_notebook = input_notebook
    output_notebook = {% raw %}"{{ ti.xcom_pull(task_ids='" + task_id + "',key='output_file_path') }}"{% endraw %}
    notebook_args = [input_notebook, output_notebook]
    parameters_str = '''
    {}
    '''.format(',\n'.join(parameter)).replace('\n', '')
    # Check if parameters is not empty or contains only whitespace
    if parameters_str.strip():  
        notebook_args.extend(["--parameters", parameters_str])
    return notebook_args

time_zone = '{{time_zone}}'
stop_cluster_check = '{{stop_cluster}}'
{{execution_order}}
{% set task_ids = [] %} 
{% for id in execution_order %}
{% set data = input_files[id] %}
input_file = '{{data["data"]["inputFile"]}}'
parameter = {{data["data"]["parameter"]}}

write_output_task_{{ id }} = PythonOperator(
    task_id='generate_output_file_{{ id }}',
    python_callable=write_output_to_file,
    provide_context=True,  
    op_kwargs={'run_id': {% raw %}'{{run_id}}'{% endraw %}, 'task_id':'generate_output_file_{{ id }}'},  
    dag=dag
)

submit_pyspark_job_{{ id }} = DataprocSubmitJobOperator(
    task_id='submit_pyspark_job_{{ id }}',
    project_id='{{gcpProjectId}}',  # This parameter can be overridden by the connection
    region='{{gcpRegion}}',  # This parameter can be overridden by the connection 
    job={
        'reference': {'project_id': '{{gcpProjectId}}'},
        'placement': {'cluster_name': '{{cluster_name}}'},
        'labels': {'client': 'dataproc-jupyter-plugin'},
        'pyspark_job': {
            'main_python_file_uri': '{{inputFilePath}}',
            'args' : get_notebook_args(input_file,parameter,'generate_output_file_{{ id }}')
        },
    },
    retries = {{data["data"]["retryCount"]}},
    gcp_conn_id='google_cloud_default',  # Reference to the GCP connection
    dag=dag,
)
{% set _ = task_ids.append('write_output_task_' + id|string) %}
{% set _ = task_ids.append('submit_pyspark_job_' + id|string) %}
{% endfor %}

{{ task_ids | join(' >> ') }}




